# tinyGPT
GPT-2 implementation using PyTorch


# Tasks
## Model
A. Transformer architecture
0. Build Attention components
    - Scaled dot-product attention
    - Multi-head attention
    - Position wise FF network
    - Masking
1. Build Embeddings
    - Token embeddings
    - position encoding
2. Build Encoder
    - Emcoder stack
3. Build Decoder
    - Decoder stack
    - Softmax output

## Data
1. Data Loader class
2. Masking
    - padding mask
    - subsequent mask